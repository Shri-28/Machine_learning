{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:210\n",
      "n_train:168\n",
      "n_test:42\n",
      "no of classes:4\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[0.64644978 0.08821934 0.42619602 0.1272661  0.08832369 0.79297503\n",
      "  0.85208279]\n",
      " [0.60139249 0.2805683  0.99047034 0.28996467 0.52607964 0.60286422\n",
      "  0.57498193]\n",
      " [0.3909599  0.94903979 0.82155062 0.54065221 0.39882219 0.79377607\n",
      "  0.09282821]\n",
      " [0.30033627 0.57183322 0.33654899 0.03076143 0.68189524 0.7778273\n",
      "  0.66355616]\n",
      " [0.29848388 0.71210762 0.87429258 0.60735683 0.26587181 0.02770898\n",
      "  0.43943075]\n",
      " [0.38881983 0.75300399 0.43736455 0.01585087 0.50740803 0.88763771\n",
      "  0.06953431]\n",
      " [0.67671944 0.6050559  0.63286989 0.23797289 0.15712952 0.60469303\n",
      "  0.56911964]\n",
      " [0.34788018 0.28475219 0.40774432 0.41600731 0.81454874 0.727612\n",
      "  0.05150223]\n",
      " [0.05277646 0.8510898  0.17454477 0.50969363 0.5753968  0.32538315\n",
      "  0.47926129]\n",
      " [0.46806841 0.36217679 0.89741564 0.91275436 0.27484727 0.43604762\n",
      "  0.63672931]\n",
      " [0.27605453 0.83044869 0.82252082 0.48178624 0.81245333 0.1211207\n",
      "  0.03234785]\n",
      " [0.39293372 0.11722366 0.25770605 0.94220207 0.57234825 0.44080568\n",
      "  0.13871616]\n",
      " [0.33778923 0.66810558 0.12907162 0.60903163 0.64523237 0.35533691\n",
      "  0.32094833]\n",
      " [0.97522518 0.70651145 0.38818547 0.50208038 0.86277594 0.83432097\n",
      "  0.69626446]\n",
      " [0.39050803 0.14417076 0.35671664 0.17737603 0.26770983 0.75874223\n",
      "  0.03705367]\n",
      " [0.60425285 0.41542622 0.68493988 0.28812796 0.76008623 0.48370925\n",
      "  0.92178012]\n",
      " [0.77591972 0.91746087 0.54287408 0.91079301 0.15635438 0.22807297\n",
      "  0.06058849]\n",
      " [0.49813829 0.7684653  0.79492072 0.35421944 0.79001808 0.91416578\n",
      "  0.03007012]\n",
      " [0.03387222 0.76324764 0.76067719 0.10341945 0.79341086 0.09781665\n",
      "  0.68353799]\n",
      " [0.47443921 0.12040682 0.83920337 0.08800393 0.19679965 0.21301748\n",
      "  0.47566793]]\n",
      "loss in 0th epoch is 251.97223367436783\n",
      "loss in 100th epoch is 251.9715625728718\n",
      "loss in 200th epoch is 251.97085097515748\n",
      "loss in 300th epoch is 251.97009470720965\n",
      "loss in 400th epoch is 251.96928897216867\n",
      "loss in 500th epoch is 251.96842822701942\n",
      "loss in 600th epoch is 251.96750602848414\n",
      "loss in 700th epoch is 251.96651483871798\n",
      "loss in 800th epoch is 251.96544577796942\n",
      "loss in 900th epoch is 251.96428830645368\n",
      "loss in 1000th epoch is 251.96302981054075\n",
      "loss in 1100th epoch is 251.96165505781502\n",
      "loss in 1200th epoch is 251.96014546969914\n",
      "loss in 1300th epoch is 251.95847813601745\n",
      "loss in 1400th epoch is 251.95662445776262\n",
      "loss in 1500th epoch is 251.95454824315806\n",
      "loss in 1600th epoch is 251.9522029812502\n",
      "loss in 1700th epoch is 251.94952784589518\n",
      "loss in 1800th epoch is 251.94644168173608\n",
      "loss in 1900th epoch is 251.94283367315313\n",
      "Mean Accuracy:0.000%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset,train_perc=0.8):\n",
    "    #np.random.shuffle(dataset)\n",
    "    data_len=len(dataset)\n",
    "    print('length of dataset:'+str(data_len))\n",
    "    train_index=int(data_len*train_perc)#last index from the dataset array that will go into training\n",
    "    train=dataset[:train_index,:]\n",
    "    test=dataset[train_index:,:]\n",
    "    return(train,test)\n",
    "\n",
    "#np.exp calculates e to the powerof input array\n",
    "def sigmoid(activation):\n",
    "    return 1.0/(1.0+np.exp(-activation))\n",
    "\n",
    "\n",
    "def compute_loss(prediction,actual):\n",
    "    #return-sum(actual*log(prediction))\n",
    "    #print(actual.T_prediction)\n",
    "    return 0.5*np.sum((actual.T-prediction)*(actual.T-prediction))\n",
    "\n",
    "\n",
    "def back_prop(train_X,W1,W2,layer1_output,layer2_output,actual_output):\n",
    "    #find error in output unit\n",
    "    difference=actual_output.T-layer2_output\n",
    "    delta_output=layer2_output*(1-layer2_output)*difference\n",
    "    delta_hidden=layer1_output*(1-layer1_output)*W2.T.dot(delta_output)\n",
    "    deltaW2=lr*(delta_output.dot(layer1_output.T)/n_train)\n",
    "    deltaW1=lr*(delta_hidden.dot(train_X)/n_train)\n",
    "    return(deltaW1,deltaW2)\n",
    "\n",
    "def train_network(train_X,train_y):\n",
    "    n_input=train_X.shape[1]\n",
    "    W1=np.random.random((n_hidden,n_input))\n",
    "    print(W1)\n",
    "    W2=np.random.random((num_classes,n_hidden))\n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        layer1_output=sigmoid(W1.dot(train_X.T))\n",
    "        #print((W1.dot(train_X.T))\n",
    "        #print(layer1_output)\n",
    "        layer2_output=sigmoid(W2.dot(layer1_output))\n",
    "        (deltaW1,deltaW2)=back_prop(train_X,W1,W2,layer1_output,layer2_output,train_y)\n",
    "        W2=W2+deltaW2\n",
    "        W1=W1+deltaW1\n",
    "        if epoch%100==0:\n",
    "            loss=compute_loss(layer2_output,train_y)\n",
    "            print(str.format('loss in {0}th epoch is {1}',epoch,loss))\n",
    "    return(W1,W2)\n",
    "\n",
    "\n",
    "def evaluate(test_X,test_y,params):\n",
    "    (W1,W2)=params\n",
    "    layer1_output=sigmoid(W1.dot(test_X.T))\n",
    "    final=sigmoid(W2.dot(layer1_output))\n",
    "    prediction=final.argmax(axis=0)\n",
    "    return np.sum(prediction==test_y)/len(test_y)\n",
    "\n",
    "\n",
    "def convert_to_OH(data,num_classes):\n",
    "    #create an array to store the one hot vectors\n",
    "    one_hot=np.zeros((len(data),num_classes))\n",
    "    one_hot[np.arange(len(data)),data]=1\n",
    "    print(one_hot)\n",
    "    return one_hot\n",
    "\n",
    "#load and prepare data\n",
    "filename='seeds_dataset.csv'\n",
    "\n",
    "df=pd.read_csv(filename,dtype=np.float64,header=None)\n",
    "\n",
    "dataset=np.array(df)\n",
    "\n",
    "(train,test)=split_dataset(dataset)\n",
    "\n",
    "n_train=len(train)\n",
    "\n",
    "print('n_train:'+str(n_train))\n",
    "\n",
    "n_test=len(test)\n",
    "\n",
    "print('n_test:'+str(n_test))\n",
    "\n",
    "lr=0.2\n",
    "\n",
    "n_epoch=2000\n",
    "\n",
    "n_hidden=20\n",
    "\n",
    "num_classes=len(np.unique(dataset[:,-1]))\n",
    "\n",
    "print('no of classes:'+str(num_classes))\n",
    "\n",
    "train_one_hot=convert_to_OH(train[:,-1].astype(int),num_classes)\n",
    "\n",
    "params=train_network(train[:,:-1],train_one_hot)\n",
    "\n",
    "accuracy=evaluate(test[:,:-1],test[:,-1],params)*100\n",
    "\n",
    "print('Mean Accuracy:%.3f%%'%accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
